Google的三驾马车读后感
19301015 潘雨祺
Google在2003到2006年间发表了三篇论文，《MapReduce: Simplified Data Processing on Large Clusters（映射简化：大型集群上的简化数据处理）》，《Bigtable: A Distributed Storage System for Structured Data（表级别分布式储存：结构化数据的分布式存储系统）》和《The Google File System（谷歌文件系统）》介绍了Google如何对大规模数据进行存储和分析。这三篇论文开启了工业界的大数据时代，被称为Google的三驾马车。
在21世纪初，互联网上的内容，大多数企业需要存储的数据量并不大。但是Google不同，Google的搜索引擎的数据基于爬虫，而由于网页的大量增加，爬虫得到的数据也随之急速膨胀，单机或简单的分布式方案已经不能满足业务的需求。因此，在文件系统的基础上更进一步，Google开发了Bigtable作为数据库，向上层服务提供基于内容的各种功能。此外，Google 的搜索结果依赖于PageRank算法的排序，而该算法又需要一些额外的数据，比如某网页的被引用次数，所以他们还开发了对于的数据处理工具MapReduce，在读取了Bigtable数据的技术上，根据业务需求，对数据内容进行运算。其总体架构如下，GFS能充分利用多个Linux服务器的磁盘，并向上掩盖分布式系统的细节。Bigtable在GFS的基础上对数据内容进行识别和存储，向上提供类似数据库的各种操作。MapReduce则使用Bigtable中的数据进行运算，再提供给具体的业务使用。

一、	Google File System
据概念介绍来，看GFS是一个可扩展的分布式文件系统，用于大型的、分布式的、对大量数据进行访问的应用。它运行于廉价的普通硬件上，并提供容错功能，给大量的用户提供总体性能较高的服务。
GFS即谷歌文件系统（Google File System），是三驾马车中最底层的组件，同时也是最复杂的。在对它进行设计前，谷歌给出了以下目标与前提：由于使用的机器是廉价的商业化机器，那么机器崩溃被认为是一种常态。系统存储以大文件为主，但也支持小文件。系统需要支持大规模的连续读取和小规模的随机读取，以及大规模的追加写。高性能稳定的网络带宽比延迟更重要。以及最重要的，能在分布式的系统上运行。
具体表现为：
⒈ 部件错误不再被当作异常，而是将其作为常见的情况加以处理。因为文件系统由成百上千个用于存储的机器构成，而这些机器是由廉价的普通部件组成并被大量的客户机访问。部件的数量和质量使得一些机器随时都有可能无法工作并且有一部分还可能无法恢复。所以实时地监控、错误检测、容错、自动恢复对系统来说必不可少。
⒉按照传统的标准，文件都非常大。每个文件通常包含很多应用对象，而当经常要处理快速增长的、包含数以万计的数据集时，我们很难管理成千上万的文件块，即使底层文件系统提供支持。因此，设计中操作的参数、块的大小必须要重新考虑。对大型的文件的管理一定要能做到高效，对小型的文件也必须支持，但不必优化。
⒊大部分文件的更新是通过添加新数据完成的，而不是改变已存在的数据。一些数据可能组成一个大仓库以供数据分析程序扫描。有些是运行中的程序连续产生的数据流。有些是档案性质的数据，有些是在某个机器上产生、在另外一个机器上处理的中间数据。由于这些对大型文件的访问方式，添加操作成为性能优化和原子性保证的焦点。而在客户机中缓存数据块则失去了吸引力。
⒋工作量主要由两种读操作构成：对大量数据的流方式的读操作和对少量数据的随机方式的读操作。前者来自同一个客户的连续操作通常会读文件的一个连续的区域。后者通常在一个随机的偏移处读几个KB。性能敏感的应用程序通常将对少量数据的读操作进行分类并进行批处理以使得读操作稳定地向前推进，而不要让它来反复的读。等等。
而在总体架构中，GFS采用了单Master的设计来简化系统的复杂度。Master负责两点，一是存储和维护数据库服务端和数据块的相关信息，二是处理客户端的请求。也就是说，Master并不存储任何具体的数据，这些数据被存在被称为数据块服务端的数据节点上。
其Master有以下功能：
1.名字空间管理和加锁：与传统文件系统不同的是，GFS没有与每个目录相关的能列出其所有文件的数据结构，它也不支持别名，不管是对文件或是目录。GFS的名字空间逻辑上是从文件元数据到路径名映射的一个查用表。
2. 备份存储放置策略：一个GFS集群文件系统可能是多层分布的。一般情况下是成千上万个文件块 服务器分布于不同的机架上，而这些文件块服务器又被分布于不同机架上的客户来访问。因此，不同机架上的两台机器之间的通信可能通过一个或多个交换机。数据块冗余配置策略要达到连个目的：最大的数据可靠性和可用性，最大的网络带宽利用率。因此，如果仅仅把数据的拷贝置于不同的机器上很难满足这两个要求，必须在不同的机架上进行数据备份。即使整个机架被毁或是掉线，也能确保数据的正常使用。 
3. 垃圾收集：在一个文件被删除之后，GFS并不立即收回磁盘空间，而是等到垃圾收集程序在文件和数据块级的的检查中收回。当一个文件被应用程序删除之后，MASTER会立即记录下这些变化，但文件所占用的资源却不会被立即收回，而是重新给文件命了一个隐藏的名字，并附上了删除 的时间戳。在MASTER定期检查名字空间时，它删除超过三天的隐藏的文件。在此之前，可以以一个新的名字来读文件，还可以以前的名字恢复。当隐藏的文件在名字空间中被删除以后，它在内存中的元数据即被擦除，这就有效地切断了他和所有数据块的联系。
……
GFS的设计是为了满足谷歌迅速增长的数据处理需求，它与过去的分布式文件系统拥有许多相同的目标，例如性能、可伸缩性、可靠性以及可用性。然而，它的设计还受到谷歌应用负载和技术环境的影响可以说，它是为谷歌应用程序本身而设计的。
二、	Big Table
Bigtable是在谷歌文件系统的基础上实现的，它关心数据的内容，根据的数据的内容建立数据模型，对外提供读写数据的接口。
Bigtable基本的数据结构和关系型数据库类似，都是以行列构成的表，但是，它还另外增加了新的维度——时间。也就是说，在行列确定的情况下，一个单元格中有多个以事件为版本的数据。
由于Bigtable是分布式的数据库，在节点之间的协调上需要额外的处理，这里，它使用了Google内部的Chubby，为了加快数据的查找和存储效率，Bigtable在存储数据之前都进行了排序。在Bigtable中，由于单个表（Table）存储的数据可能相当地多，那么读写的效率就会十分低下，于是Bigtable将Table分割为固定大小的Tablet，将其作为数据存储和查找的基本单位。每当Table增加了这里要说明的是，tablet是数据存储的基本单元，是用户感知不到的。
而其特点相似于其余两者，适合大规模海量数据，PB级数据，因为使用分布式、并发数据处理，效率极高；同时易于扩展，支持动态伸缩；由于数据结构的性质，它适合于读操作，不适合写操作，同样也不适用于传统关系型数据库。
此外，Bigtable数据库的架构，由主服务器和分服务器构成。如果我们把数据库看成是一张大表，那么可将其划分为许多基本的小表，这些小表就称为tablet，是bigtable中最小的处理单位了。主服务器负责将Tablet分配到Tablet服务器、检测新增和过期的Tablet服务器、平衡Tablet服务器之间的负载、GFS垃圾文件的回收、数据模式的改变等。Tablet服务器负责处理数据的读写，并在Tablet规模过大时进行拆分。

三、	MapReduce
尝试解决利用大数据进行计算时，使用这种函数式编程中的两个函数——思想简化计算模型。MapReduce把所有的计算都拆分成两个基本的计算操作，即Map（映射）和Reduce（归约）。其中Map函数以一系列键值对作为输入，然后输出一个中间文件。这个中间态是另一种形式的键值对。然后，Reduce函数将这个中间态作为输入，计算得出结果。
简单说来，一个映射函数就是对一些独立元素组成的概念上的列表(例如，一个测试成绩的列表)的每一个元素进行指定的操作(比如前面的例子里，有人发现所有学生的成绩都被高估了一分，它可以定义一个"减一"的映射函数，用来修正这个错误。)。事实上，每个元素都是被独立操作的，而原始列表没有被更改，因为这里创建了一个新的列表来保存新的答案。这就是说，Map操作是可以高度并行的，这对高性能要求的应用以及并行计算领域的需求非常有用。而化简操作指的是对一个列表的元素进行适当的合并。虽然不如映射函数那么并行，但是因为化简总是有一个简单的答案，大规模的运算相对独立，所以化简函数在高度并行环境下也很有用。
MapReduce主要有以下功能：
一，	数据划分和计算任务调度:系统自动将一个作业待处理的大数据划分为很多个数据块，每个数据块对应于一个计算任务，并自动 调度计算节点来处理相应的数据块。作业和任务调度功能主要负责分配和调度计算节点，同时负责监控这些节点的执行状态，并负责Map节点执行的同步控制。
二，	数据/代码互定位:为了减少数据通信，一个基本原则是本地化数据处理，即一个计算节点尽可能处理其本地磁盘上所分布存储的数据，这实现了代码向 数据的迁移;当无法进行这种本地化数据处理时，再寻找其他可用节点并将数据从网络上传送给该节点，但将尽可能从数据所在的本地机架上寻 找可用节点以减少通信延迟。
三，	系统优化:为了减少数据通信开销，中间结果数据进入Reduce节点前会进行一定的合并处理；一个Reduce节点所处理的数据可能会来自多个 Map节点，为了避免简化计算阶段发生数据相关性，Map节点输出的中间结果需使用一定的策略进行适当的划分处理，保证相关性数据发送到同一个 Reduce节点；此外，系统还进行一些计算性能优化处理，如对最慢的计算任务采用多备份执行、选最快完成者作为结果。
四，	出错检测和恢复:以低端商用服务器构成的大规模计算集群中，节点硬件出错和软件出错是常态，因此 MapReduce需要能检测并隔离出错节点，并调度分配新的节点接管出错节点的计算任务。同时，系统还将维护数据存储的可靠性，用多备份冗余存储机制提 高数据存储的可靠性，并能及时检测和恢复出错的数据。
而在技术方面，MapReduce具有向"外"横向扩展，而非向"上"纵向扩展的特点。集群的构建完全选用价格便宜、易于扩展的低端商用服务器，而非价格昂贵、不易扩展的高端服务器。对于大规模数据处理，由于有大量数据存储的需要，则基于低端服务器的集群远比基于高端服务器的集群优越。也正因为选用大量低端处理器，它的失效被认为是常态。为了解决这种现象带来的问题，MapReduce并行计算软件框架使用了多种有效的错误检测和恢复机制，如节点自动重启技术，使集群和计算框架具有对付节点失效的健壮性，能有效处理失效节点的检测和恢复，这也使它具有了高容错的特点。
而为了减少大规模数据并行计算系统中的数据通信开销，代之以把数据传送到处理节点，应当考虑将处理向数据靠拢和迁移。它采用了数据/代码互定位的技术方法，计算节点将首先尽量负责计算其本地存储的数据，以发挥数据本地化特点，仅当节点无法处理本地数据时，再采用就近原则寻找并传输数据到其他可用计算节点。这被称为将处理向数据迁移。
同时，为了实现面向大数据集批处理的高吞吐量的并行处理，MapReduce可以利用集群中的大量数据存储节点同时访问数据，以此利用分布集群中大量节点上的磁盘集合提供高带宽的数据访问和传输，避免了随机访问数据。
MapReduce提供了一种抽象机制将程序员与系统层细节隔离开来，程序员仅需描述需要计算什么，而具体怎么去计算就交由系统的执行框架处理，这样程序员可从系统层细节中解放出来，而致力于其应用本身计算问题的算法设计。
总而言之，MapReduce之所以能成功，是因为它的库封装了并行处理、容错处理、本地数据优化、负载均衡等技术难点细节，使得库易于使用；大量需求能够通过这种模型解决：检索、排序、挖掘、机器学习等；实现并部署了千台机器组成的MapReduce，解决了很多问题。在大数据计算十分火爆的现在，MapReduce模型作为一种高效的分布式计算模型，面对日益增加的大规模数据的计算任务，它将会发挥更重要的作用。
读完“三驾马车”，我也对分布式处理有了一些粗浅的了解。Bigtable重要的贡献是证明了在分布式的系统中，针对超大规模的数据量，使用排序大表的来设计数据库是可行的。尽管距离这三篇论文发表已经过去了十数年，它们仍是如今学习分布式管理时十分值得研学的作品。
